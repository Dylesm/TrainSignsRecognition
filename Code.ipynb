{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["rfwcthZYfP5g","Fw6i4tYOfsrN","jzp-yErmgEqP","KMeU6syngjoq","26X6nH1rxRdn","Wau91_TRxT5v","PT-2xEpLe9t7","YCZsjcJ16IFM","I4L-qhm_e8fx","erMuTiEFfDGf","yJOI46ImfFDi","wiSspkPSfJK7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Documentation\n","In this notebook, we provide all the code required to create the Railway recognition pipeline, which you can explore in the accompanying demo notebook located in this folder. This document serves as a comprehensive reference, presenting methods and their brief documentation to offer insights into our development process. Please note that this file does not contain any logical sequence for execution, and running the code within it will not yield meaningful results. Its sole purpose is documentation."],"metadata":{"id":"nA21O2az5Fn1"}},{"cell_type":"markdown","source":["# All Imports"],"metadata":{"id":"rfwcthZYfP5g"}},{"cell_type":"code","source":["# Install necessary packages\n","!pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu117/torch1.13.0/index.html\n","!pip install mmdet==2.26.0\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html\n","!pip install transformers timm\n","!pip install mmdet\n","!pip install -U openmim\n","!pip install -U torch sahi\n","!pip install ultralytics==8.0.202\n","!pip install tqdm --upgrade\n","\n","# Standard library imports\n","import os\n","from tqdm.notebook import tqdm  # Progress bar for loops\n","import PIL  # Python Imaging Library for image processing\n","\n","# Custom package imports\n","from sahi.utils.yolov8 import download_yolov8s_model  # YOLOv8 model download utility\n","from sahi import AutoDetectionModel  # Auto Detection Model from Sahi\n","from sahi.utils.cv import read_image  # Read image utility\n","from sahi.utils.file import download_from_url  # Download file from URL utility\n","from sahi.predict import get_prediction, get_sliced_prediction, predict  # Prediction utilities\n","from IPython.display import Image  # Display images in Jupyter Notebooks\n","\n","# Image processing imports\n","from PIL import ImageEnhance, ImageFilter  # Image enhancement and filtering\n","from IPython import display  # Display utilities\n","\n","# Deep learning framework and computer vision libraries\n","from ultralytics import YOLO  # YOLO object detection from Ultralytics\n","import shutil  # File operations utility\n","import random  # Random number generation\n","import cv2  # OpenCV for computer vision tasks\n","import math  # Mathematical functions\n","\n","# Google Colab specific imports\n","from google.colab import drive  # Google Drive integration\n","import pandas as pd  # Data manipulation library\n","import numpy as np  # Numerical computing library\n","\n","# Set the current working directory and print it\n","HOME = os.getcwd()\n","print(HOME)"],"metadata":{"id":"-hdQrZL1fTuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data preprocessing\n"],"metadata":{"id":"kT9wAndDeyv0"}},{"cell_type":"markdown","source":["## Resizing\n"],"metadata":{"id":"Fw6i4tYOfsrN"}},{"cell_type":"code","source":["# As input takes folder with full size images, and destination where to export data\n","def resizeImags(path,destin):\n","  for n in tqdm(os.listdir(path)):\n","    print(n[-4:]=='.tif')\n","    if n[-4:] == \".tif\":\n","      img = cv2.imread(path+ n)\n","\n","      print('Original Dimensions : ',img.shape)\n","      scale_percent = 20 # percent of original size\n","      width = int(img.shape[1] * scale_percent / 100)\n","      height = int(img.shape[0] * scale_percent / 100)\n","      dim = (width, height)\n","      # resize image\n","      resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n","      print('Resized Dimensions : ',resized.shape)\n","      cv2.imwrite(destin+n, resized)"],"metadata":{"id":"PJi_VadTfu8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pairing labels with images"],"metadata":{"id":"jzp-yErmgEqP"}},{"cell_type":"code","source":["#takes folder with annotations and folder with images\n","def pairImages(annot,destin):\n","  goodlist = []\n","  for anno in tqdm(os.listdir(annot)):\n","\n","    anno = anno[4:]\n","    print(anno)\n","    newname = anno[:-4]+\".tif\"\n","    if newname in os.listdir(destin):\n","      goodlist.append(anno[:-4]+\".tif\")\n","      img = cv2.imread(destin+newname)\n","      cv2.imwrite(annot+newname, img)"],"metadata":{"id":"vE-_4-2BgEw1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Splitting Train/Validation"],"metadata":{"id":"KMeU6syngjoq"}},{"cell_type":"code","source":["# Define folder paths for datasets\n","train_path_img =\"/content/drive/MyDrive/Railway/correct_stuff/augmented/train/\"\n","train_path_label = \"/content/drive/MyDrive/Railway/correct_stuff/augmented/labels/train/\"\n","val_path_img = \"/content/drive/MyDrive/Railway/correct_stuff/augmented/images/val/\"\n","val_path_label = \"/content/drive/MyDrive/Railway/correct_stuff/augmented/labels/val/\"\n","test_path = \"/content/drive/MyDrive/Railway/correct_stuff/augmented/test\""],"metadata":{"id":"UhGUg9u1hjhk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_test_split(path,neg_path=None, split = 0.2):\n","    print(\"------ PROCESS STARTED -------\")\n","\n","\n","    files = list(set([name[:-4] for name in os.listdir(path)])) ## removing duplicate names i.e. counting only number of images\n","\n","\n","\n","    print(files)\n","    print (f\"--- This folder has a total number of {len(files)} images---\")\n","    random.seed(42)\n","    random.shuffle(files)\n","\n","    test_size = int(len(files) * split)\n","    train_size = len(files) - test_size\n","\n","    ## creating required directories\n","\n","    os.makedirs(train_path_img, exist_ok = True)\n","    os.makedirs(train_path_label, exist_ok = True)\n","    os.makedirs(val_path_img, exist_ok = True)\n","    os.makedirs(val_path_label, exist_ok = True)\n","\n","\n","    ### ----------- copying images to train folder\n","    for filex in tqdm(files[:train_size]):\n","      print(filex)\n","      if filex == 'classes':\n","          continue\n","      shutil.copy2(path + filex + '.tif',f\"{train_path_img}/\" + filex + '.tif' )\n","      shutil.copy2(path + filex + '.txt', f\"{train_path_label}/\" + filex + '.txt')\n","\n","\n","\n","    print(f\"------ Training data created with 80% split {len(files[:train_size])} images -------\")\n","\n","    if neg_path:\n","        neg_images = list(set([name[:-4] for name in os.listdir(neg_path)])) ## removing duplicate names i.e. counting only number of images\n","        for filex in tqdm(neg_images):\n","            shutil.copy2(neg_path+filex+ \".jpg\", f\"{train_path_img}/\" + filex + '.jpg')\n","\n","        print(f\"------ Total  {len(neg_images)} negative images added to the training data -------\")\n","\n","        print(f\"------ TOTAL Training data created with {len(files[:train_size]) + len(neg_images)} images -------\")\n","\n","\n","\n","    ### copytin images to validation folder\n","    for filex in tqdm(files[train_size:]):\n","      if filex == 'classes':\n","          continue\n","      # print(\"running\")\n","      shutil.copy2(path + filex + '.tif', f\"{val_path_img}/\" + filex + '.tif' )\n","      shutil.copy2(path + filex + '.txt', f\"{val_path_label}/\" + filex + '.txt')\n","\n","    print(f\"------ Testing data created with a total of {len(files[train_size:])} images ----------\")\n","\n","    print(\"------ TASK COMPLETED -------\")\n"],"metadata":{"id":"4H0leVaegjtI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Augmentation"],"metadata":{"id":"jQud4kQGe_e8"}},{"cell_type":"markdown","source":["### Images reflection"],"metadata":{"id":"26X6nH1rxRdn"}},{"cell_type":"code","source":["# Mirror the boundry boxes\n","def mirror_labels(array):\n","  return [array[0], 1 - float(array[1]), float(array[2]), float(array[3]), float(array[4])]\n","\n","def mirror(filename, read_from, save_to, test):\n","\n","  file_path = os.path.join(read_from, filename)\n","  image = cv2.imread(file_path)\n","\n","\n","  # File the image and save it\n","  mirrored_image = cv2.flip(image, 1)\n","  mirrored_image_path = os.path.join(save_to, \"mirrored_\" + filename)\n","  cv2.imwrite(mirrored_image_path, mirrored_image)\n","\n","\n","  output = []\n","\n","\n","  with open(file_path[:-4] + '.txt') as file:\n","    for line in file:\n","      words = line.split()\n","\n","      # mirror all of the boundry boxes\n","      new_labels = mirror_labels(words)\n","\n","\n","      for word in new_labels:\n","        output.append(word)\n","\n","  # Save the boundry boxes\n","  with open(save_to + 'mirrored_' + filename[:-4] + '.txt', \"w\") as file:\n","      for index, item in enumerate(output, start=1):\n","        file.write(str(item) + ' ')\n","        if index % 5 == 0 and index != 0 and index != len(output):\n","          file.write(\"\\n\")\n"],"metadata":{"id":"6taXimhjfCv7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Brightness, Contrast, Gaussian Blur augmentation"],"metadata":{"id":"Wau91_TRxT5v"}},{"cell_type":"code","source":["def augmentData(data, destination):\n","    images = list(set([name[:-4] for name in os.listdir(data)]))\n","    images = images[89:]\n","    for i in tqdm(images):\n","        # Open the original image\n","        original_image = Image.open(f\"{data}/{i}.tif\")\n","\n","        # Save the original image and text file in the new folder\n","        original_image.save(f\"{destination}/original_{i}.tif\")\n","        shutil.copy2(f\"{data}/{i}.txt\", f\"{destination}/original_{i}.txt\")\n","\n","        # Brightness augmentation\n","        for brightness_factor in [0.5, 1.5]:\n","            brightened_image = ImageEnhance.Brightness(original_image).enhance(brightness_factor)\n","            brightened_image.save(f\"{destination}/bright_{brightness_factor}_{i}.tif\")\n","            shutil.copy2(f\"{data}/{i}.txt\", f\"{destination}/bright_{brightness_factor}_{i}.txt\")\n","\n","        # Contrast augmentation\n","        for contrast_factor in [0.5, 1.5]:\n","            contrasted_image = ImageEnhance.Contrast(original_image).enhance(contrast_factor)\n","            contrasted_image.save(f\"{destination}/contrast_{contrast_factor}_{i}.tif\")\n","            shutil.copy2(f\"{data}/{i}.txt\", f\"{destination}/contrast_{contrast_factor}_{i}.txt\")\n","\n","        # Gaussian Blur augmentation\n","        for blur_radius in [1, 2, 3]:\n","            blurred_image = original_image.filter(ImageFilter.GaussianBlur(blur_radius))\n","            blurred_image.save(f\"{destination}/blur_{blur_radius}_{i}.tif\")\n","            shutil.copy2(f\"{data}/{i}.txt\", f\"{destination}/blur_{blur_radius}_{i}.txt\")\n","    print('---Data Augmented---')\n"],"metadata":{"id":"S44RBNAoxUAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OCPiAbq1yA-o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cropping training tracks"],"metadata":{"id":"PT-2xEpLe9t7"}},{"cell_type":"code","source":["\n","\n","#gives back list of predictions, takes as argumetns path to flder with images and path to yolo model\n","def predictAndExport(folder,path):\n","  model = YOLO(path)\n","  results = []\n","\n","  for n in tqdm(os.listdir(folder)):\n","      if n[-4:] == \".tif\" :\n","        objectPred = {}\n","        img = Image.open(folder+\"/\"+ n)\n","        print(n)\n","        result = model.predict(img,imgsz=1024, conf=0.5)\n","        result = result[0]\n","        objectPred['name'] = n\n","        if len(result.boxes.cls.cpu().numpy()) == 0:\n","          pass\n","        else:\n","          objectPred['label'] = int(math.floor(result.boxes.cls.cpu().numpy()[0]))\n","          objectPred['conf'] = result.boxes.conf.cpu().numpy()[0]\n","          objectPred['bBox0'] = result.boxes.xywhn.cpu().numpy()[0][0]\n","          objectPred['bBox1'] = result.boxes.xywhn.cpu().numpy()[0][2]\n","          print(objectPred)\n","          results.append(objectPred)\n","  return results\n","\n","\n"],"metadata":{"id":"EBv3WF8De-Ph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Writes a csv with the predictions"],"metadata":{"id":"ipTnNbZ5nCk-"}},{"cell_type":"code","source":["def writeACSV(destin,data,filename):\n","  columns = [\"name\", \"conf\", \"label\", \"bBox0\",\"bBox1\"]\n","  with open(destin+filename+\".csv\", mode=\"w\", newline=\"\") as file:\n","    writer = csv.DictWriter(file, fieldnames=columns)\n","    # Write the header\n","    writer.writeheader()\n","    # Write the data rows\n","    for row in tqdm(data):\n","        print(row)\n","        writer.writerow(row)"],"metadata":{"id":"lOzTVTAjnCpM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Actual cropping"],"metadata":{"id":"YCZsjcJ16IFM"}},{"cell_type":"code","source":["def crop_images(annots, imagesDir, outputfolderdir):\n","\n","  df = pd.read_csv(annots)\n","\n","  df = df.tail(-1)\n","\n","  df = df\n","  listimages = os.listdir(imagesDir+\"/\")\n","  print(listimages)\n","  for row in tqdm(df.iterrows()):\n","\n","    row = row[1]\n","    if row[\"name\"] in listimages:\n","      print(row[\"name\"])\n","\n","      image = cv2.imread(imagesDir + '/' + str(row['name']))\n","\n","\n","      cropped_image = crop_image(row['bBox0'], row['bBox1'] ,image)\n","      #print(cropped_image)\n","      print(imagesDir + '/' + str(row['name']))\n","\n","\n","      try:\n","        cv2.imwrite(outputfolderdir + 'cropped_' + str(row['name']), cropped_image)\n","      except:\n","        print(cropped_image)\n","\n","\n","def crop_image(bBox0, bBox1,  image):\n","\n","  bBox0 = bBox0\n","  # Adds padding to the cropping\n","  bBox1 = bBox1 * 1.2\n","\n","\n","  # Get the image dimensions\n","  height, width, _ = image.shape\n","\n","  # Calculate the bounding box coordinates in pixel values\n","\n","  x1 = int((bBox0 - bBox1/2)*width ) if x1>= 0 else 0\n","  x2 = int((bBox0 + bBox1/2)*width) if x2 <= width else width\n","\n","  # Crop the image\n","  cropped_image = image[0:height, x1:x2]\n","  return cropped_image\n"],"metadata":{"id":"mRXAStpD6IJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"I4L-qhm_e8fx"}},{"cell_type":"code","source":["# Example training on existing model\n","model = ultralytics.YOLO('yolov8n.pt').load(\"/content/drive/MyDrive/Railway/modelv2_26/modelAugmentations/weights/last.pt\")\n","model.train( resume=True ,data='/content/drive/MyDrive/Railway/correct_stuff/dataset.yaml', epochs=25, imgsz=1024, device = 0, workers = 0)"],"metadata":{"id":"5BE3BR5Qe9Ur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example training wiht Yolov8 CLI\n","!yolo task=detect mode=train model=yolov8n.pt data=/content/drive/MyDrive/Railway/BetterStationsIMG/dataset.yaml epochs=25 imgsz=1024 batch=8 project=/content/drive/MyDrive/Railway/bettermodel/ name=trainsmodelOutput"],"metadata":{"id":"EdjKP_Px7YJB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Predicting"],"metadata":{"id":"erMuTiEFfDGf"}},{"cell_type":"code","source":["#method for predicting images from a folder using Slicing aided hyper inference\n","def SahiPredictionExport(test_path,model_path,export_dir):\n","    files = []\n","    for n in tqdm(os.listdir(test_path)):\n","      files.append(test_path + n)\n","\n","    detection_model = AutoDetectionModel.from_pretrained(\n","        model_type=\"yolov8\",\n","        model_path=model_path,\n","        confidence_threshold=0.1,\n","        device= 'cuda:0'\n","    )\n","\n","    n = 0\n","    for a in files:\n","      result = get_sliced_prediction(a, detection_model)\n","\n","      print(result)\n","      result.export_visuals(export_dir=export_dir,file_name=f\"predicted_image_{n}\")\n","      n= n+1"],"metadata":{"id":"j1SZIQC_1fxm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"yJOI46ImfFDi"}},{"cell_type":"code","source":["#code that evaluates the performance of the model on the validation dataset\n","model = YOLO(modelPath)  # load a custom model\n","\n","# Validate the model\n","metrics = model.val(imgsz=1024, save_json=True)  # no arguments needed, dataset and settings remembered\n","print(metrics.box.map)    # map50-95\n","print(metrics.box.map50)  # map50\n","print(metrics.box.map75)  # map75\n","print(metrics.box.maps)"],"metadata":{"id":"jhAO8YIsfF8o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# OCR"],"metadata":{"id":"wiSspkPSfJK7"}},{"cell_type":"code","source":["\n","# Creating OCR Reader for English Language\n","reader = easyocr.Reader(['en'])\n","\n","def recognize_sign_number(image_path):\n","  # Make the picture bigger\n","  new_image = cv2.resize(image_path, (800, 800))\n","  # apply ocr method from easyocr library, allowing only numbers from 0 to 9\n","  ocr_result = reader.readtext(new_image, allowlist='0123456789')\n","\n","  if ocr_result == []:\n","    return 'no result'\n","\n","  return str(ocr_result[0][1])"],"metadata":{"id":"8jOM_-QTfJ9k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Triangulation"],"metadata":{"id":"IpPQ2dctfKTZ"}},{"cell_type":"markdown","source":["####Open and read the location data and the camera settings"],"metadata":{"id":"QcW05WmQii45"}},{"cell_type":"code","source":["\n","def getLocation(file):\n","\n","  return  pd.read_csv(file)\n","\n","def readSettings(file):\n","  cam_settings = {}\n","  for i in range(len(os.listdir(file))):\n","\n","      # print(os.listdir(folder)[i])\n","\n","      if os.listdir(file)[i].endswith('.txt'):\n","        # print(os.listdir(file)[i][:-4])\n","        f = open(os.path.join(file, os.listdir(file)[i]))\n","        settings = f.readlines()\n","        foldname = settings[0][19:24]\n","        print(foldname)\n","        # print(settings)\n","        folds = {}\n","        for i in settings:\n","            if i.startswith('-'):\n","                # print(type(i))\n","\n","                split = i[2:-1].split(':')\n","                folds[split[0].split(' ')[1]] = split[1][1:]\n","\n","\n","        cam_settings[foldname] = folds\n","\n","  return cam_settings\n","\n"],"metadata":{"id":"FiKsrGm6fMXY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Functions"],"metadata":{"id":"pVyiyxAIi3Ok"}},{"cell_type":"code","source":["# Returns the rotation matrix\n","def get_rot_mat(omg, phi, kap):\n","    # Rotations, note: minus sign!\n","    omg = -np.deg2rad(omg)\n","    phi = -np.deg2rad(phi)\n","    kap = -np.deg2rad(kap)\n","\n","    Rx = np.array([[1,0,0],[0, np.cos(omg), -np.sin(omg)],[0, np.sin(omg), np.cos(omg)]])\n","    Ry = np.array([[np.cos(phi), 0, np.sin(phi)],[0,1,0],[-np.sin(phi), 0, np.cos(phi)]])\n","    Rz = np.array([[np.cos(kap), -np.sin(kap), 0],[np.sin(kap), np.cos(kap), 0], [0,0,1]])\n","    return Rz@Ry@Rx\n","\n","\n","\n","# Returns the intrinsic matrix\n","def get_int_mat(f, img_w, img_h):\n","    f = (f/10)/100 # focal length in m\n","    pix_size = 3.76e-6 # Physical size of a pixel on the sensor in m\n","    fc = f/pix_size # Normalised, dimensionless focal length\n","\n","    # Image offsets\n","    u0, v0 = (img_w/2), (img_h/2)\n","\n","    # Construct intrinsic matrix\n","    K = np.array([[-fc, 0, u0, 0],[0, fc, v0, 0],[0, 0, 1, 0]])\n","\n","    return K\n","\n","\n","# Returns the camera matrix\n","def get_cam_mat(R, cp, K):\n","    M = np.eye(4)\n","\n","    M[0:3,0:3] = R\n","    M[0:3,3] = -R@cp[:,0]\n","    return K@M\n","\n","# Returns the world position based on corresponding pixel pairs and the two camera matrices\n","def get_pos(pp1, pp2, P1, P2):\n","    A = np.zeros((4,4))\n","    A[0,:] = pp1[1]*P1[2,:] - P1[1,:]\n","    A[1,:] = P1[0,:] - pp1[0]*P1[2,:]\n","    A[2,:] = pp2[1]*P2[2,:] - P2[1,:]\n","    A[3,:] = P2[0,:] - pp2[0]*P2[2,:]\n","    U, S, Vh = np.linalg.svd(A, full_matrices=True)\n","    pos = Vh[3,:]/Vh[3,3]\n","    return pos[:-1]\n","\n","\n","\n","# creates the bounding boxes from the txt files obtained from model\n","def BoundingBox(dic):\n","\n","  coorList = []\n","\n","\n","  for prediction in dic:\n","    # print(prediction)\n","    nums = prediction['bbox']\n","    label = prediction['category_name']\n","\n","    coordinates = np.zeros((5,2))\n","    coordinates[0,:] = [(nums[0]+(nums[2]/2)), (nums[1]+(nums[3]/2))]#center\n","    coordinates[1,:] = [(nums[0]+nums[2]), (nums[2])]#top left corner\n","    coordinates[2,:] = [(nums[0]+(nums[2])), (nums[2]+(nums[3]))]#top right corner\n","    coordinates[3,:] = [(nums[0]), (nums[1])]#bottom left corner\n","    coordinates[4,:] = [(nums[0]), (nums[1]+(nums[3]))]#bottom right corner\n","    coorList.append([coordinates,label])\n","\n","  return coorList\n","\n","\n","# for more info about this algorithm, check https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html\n","def find_matching_points(image1, image2):\n","    # Load images\n","    img1 = cv2.imread(image1, cv2.IMREAD_GRAYSCALE)\n","    img2 = cv2.imread(image2, cv2.IMREAD_GRAYSCALE)\n","\n","    #Here, you can also used SIFT, but ORB has less computational strain\n","    orb = cv2.ORB_create()\n","\n","    # Detect keypoints and descriptors\n","    kp1, des1 = orb.detectAndCompute(img1, None)\n","    kp2, des2 = orb.detectAndCompute(img2, None)\n","\n","    # here, FLANN based matcher can be used to make it faster, however BFmatcher returns the best results\n","    # Initialize Brute Force Matcher\n","    bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n","\n","\n","    matches = bf.knnMatch(des1, des2, k=2)\n","    # print(matches)\n","\n","    # Apply ratio test\n","    good_matches = []\n","    for m, n in matches:\n","        if m.distance < 0.75 * n.distance:\n","            good_matches.append(m)\n","\n","    # Get matching points that scored high on ratio test\n","    pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1,2)\n","    pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1,2)\n","\n","    # Find homography using RANSAC\n","    H, _ = cv2.findHomography(pts1, pts2, cv2.RANSAC)\n","\n","    return H\n","\n","\n","# matches the centers of the bounding boxes of detected objects for triangulation\n","def find_matching_boxes(predictions1, predictions2,H):\n","    predictions1 = BoundingBox(predictions1)\n","    predictions2 = BoundingBox(predictions2)\n","\n","\n","    distances = np.zeros((len(predictions1), len(predictions2)))\n","\n","\n","    for i in range(len(predictions1)):\n","        for j in range(len(predictions2)):\n","          if predictions1[i][1] == predictions2[j][1]:\n","\n","            distances[i, j] = math.dist(predictions2[j][0][0],cv2.perspectiveTransform(predictions1[i][0][0].reshape(-1,1,2),H).reshape(2,))\n","    # print(distances)\n","    matches = []\n","    try:\n","        for i in range(len(predictions1)):\n","          sorted = np.argsort(distances[i])\n","          match_index = None\n","          if (distances[i][sorted[0]]) == 0 and (len(sorted) == 1):\n","            continue\n","          elif (distances[i][sorted[0]]) == 0 and (len(sorted) != 1):\n","            match_index = sorted[1]\n","          elif distances[i][sorted[0]]!= 0:\n","            match_index = sorted[0]\n","          matches.append((i, match_index, predictions1[i][0][0], predictions2[i][0][0], distances[i][match_index]))\n","\n","\n","        for i1, m1 in enumerate(matches):\n","          for i2, m2 in enumerate(matches):\n","            if (m1[1] == m2[1]) and (i2 != i1):\n","              if m1[4] < m2[4]:\n","                matches.remove(i1)\n","              elif m2[4] < m1[4]:\n","                matches.remove(i2)\n","    except:\n","      pass\n","\n","\n","    return matches\n","\n","# gets real world position after finding the pairs\n","def get_pos_matching_boxes(boxpair, im1, im2, positions, camSet):\n","  P = dict()\n","\n","  i1 = positions[positions['Filename']== im1[-20:-4]].index[0]\n","  i2=  positions[positions['Filename']== im2[-20:-4]].index[0]\n","\n","  print(camSet)\n","  print(im1)\n","  print(im1[-20:-15])\n","\n","  #getting the relevant info for constructing the camera matrices\n","  R= get_rot_mat(positions.loc[i1]['Omega'],positions.loc[i1]['Phi'],positions.loc[i1]['Kappa'])\n","  cp = np.array([[positions.loc[i1]['X/Long'],positions.loc[i1]['Y/Lat'], positions.loc[i1]['Z']]]).T\n","  K =  get_int_mat(int(float(camSet.get(im1[-20:-15]).get('focal'))),int(float(camSet.get(im1[-20:-15]).get('width'))),int(float(camSet.get(im1[-20:-15]).get('height'))))\n","  P[im1] = get_cam_mat(R,cp,K)\n","\n","  R= get_rot_mat(positions.loc[i2]['Omega'],positions.loc[i2]['Phi'],positions.loc[i2]['Kappa'])\n","  cp = np.array([[positions.loc[i2]['X/Long'],positions.loc[i2]['Y/Lat'], positions.loc[i2]['Z']]]).T\n","  K =  get_int_mat(int(float(camSet,get(im2[-20:-15]).get('focal'))),int(float(camSet.get(im2[-20:-15]).get('width'))),int(float(camSet.get(im2[-20:-15]).get('height'))))\n","  P[im2] = get_cam_mat(R,cp,K)\n","\n","\n","\n","  npoints = len(boxpair)\n","  wkast = np.zeros((npoints,3))\n","  final = []\n","\n","  for i in range(npoints):\n","      wkast[i,:] = get_pos(boxpair[i][2], boxpair[i][2], P[im1], P[im2])\n","      #add RD coordinates and index of which box it matches in image2\n","      final.append([wkast[i,0], wkast[i,1], boxpair[i][1]])\n","      # print(get_pos(boxpair[i][2], boxpair[i][2], P[im1], P[im2])) #uncomment to see the coordinates printed out\n","\n","  return final\n","\n","\n","def geoloc (image1, image2, prediction1, prediction2, path):\n","    # positions, camSet = OpenImages(path)# path to be changed\n","    # print(os.path.join(path, 'positions.csv'))\n","    positions = getLocation(path)\n","\n","    camSet = readSettings(path)\n","\n","\n","    H = find_matching_points(image1, image2)\n","    boxpair = find_matching_boxes(prediction1, prediction2,H)\n","    results = get_pos_matching_boxes(boxpair, image1, image2,positions,camSet)\n","\n","    return results\n","\n"],"metadata":{"id":"cABNrexjitZh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"ZL0-IwSVjmbJ"},"execution_count":null,"outputs":[]}]}